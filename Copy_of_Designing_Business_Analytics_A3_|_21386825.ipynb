{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMYPUW8SSlpxaYux2Ee+R0z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/goodwillhunting9/AI-Driven-Food-Security-Platform/blob/main/Copy_of_Designing_Business_Analytics_A3_%7C_21386825.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFlZLJ04KPVS"
      },
      "outputs": [],
      "source": [
        "Unzipping the file: It extracts the CSV from the ZIP file and places it in the /content/ directory.\n",
        "Loading and previewing the data: The dataset is loaded into a pandas DataFrame and the first five rows are displayed to give you a glimpse of its structure.\n",
        "Checking for missing values and duplicates: The script will identify any missing or duplicate data and handle them.\n",
        "Data types and summary statistics: This will provide insights into the data types of each column and give a summary of the numerical features.\n",
        "Class distribution check: This checks the distribution of the target variable (Class), which indicates fraudulent and non-fraudulent transactions.\n",
        "Saving the cleaned dataset: The cleaned dataset will be saved as creditcard_cleaned.csv in the /content/ directory for further analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6_yEj3BuL09s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-trpjdlJLmwv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import necessary libraries\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "\n",
        "# Step 2: Extract and load the dataset from the ZIP file\n",
        "zip_file_path = '/content/creditcard.csv.zip'  # Path to the ZIP file\n",
        "\n",
        "# Unzip the file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/')  # Extracts into the /content/ directory in Colab\n",
        "\n",
        "# Load the CSV file (assuming it is named 'creditcard.csv' after extraction)\n",
        "csv_file_path = '/content/creditcard.csv'  # Path to the extracted CSV file\n",
        "df = pd.read_csv(csv_file_path)\n",
        "\n",
        "# Step 3: Display the first few rows to understand the dataset structure\n",
        "print(\"First 5 rows of the dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "# Step 4: Check for missing values\n",
        "print(\"\\nChecking for missing values:\")\n",
        "missing_values = df.isnull().sum()\n",
        "print(missing_values[missing_values > 0])\n",
        "\n",
        "# Step 5: Handle missing values (if any) - here we drop them, but you could impute as necessary\n",
        "df_cleaned = df.dropna()  # Drop rows with missing values\n",
        "# Alternatively, you can fill missing values like this:\n",
        "# df_cleaned = df.fillna(df.median())  # Impute missing values with median\n",
        "\n",
        "# Step 6: Remove duplicate rows (if any)\n",
        "print(\"\\nChecking for duplicate rows:\")\n",
        "duplicate_rows = df_cleaned.duplicated().sum()\n",
        "print(f\"Number of duplicate rows: {duplicate_rows}\")\n",
        "df_cleaned = df_cleaned.drop_duplicates()\n",
        "\n",
        "# Step 7: Check for data types of the columns\n",
        "print(\"\\nData types of each column:\")\n",
        "print(df_cleaned.dtypes)\n",
        "\n",
        "# Step 8: Summary statistics of the numerical features\n",
        "print(\"\\nSummary statistics of numerical features:\")\n",
        "print(df_cleaned.describe())\n",
        "\n",
        "# Step 9: Inspect the class distribution (fraudulent vs non-fraudulent transactions)\n",
        "print(\"\\nClass distribution (fraud vs non-fraud):\")\n",
        "print(df_cleaned['Class'].value_counts())\n",
        "\n",
        "# Step 10: Save the cleaned dataset to a new CSV file in the /content/ directory\n",
        "output_file_path = '/content/creditcard_cleaned.csv'  # You can download this file from Colab after cleaning\n",
        "df_cleaned.to_csv(output_file_path, index=False)\n",
        "print(f\"\\nCleaned dataset saved as {output_file_path}\")\n"
      ],
      "metadata": {
        "id": "iCRsxBqKK0TN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Exploratory Data Analysis (EDA)\n"
      ],
      "metadata": {
        "id": "HJR07SMbL8YU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import necessary libraries for visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set the aesthetic style for the plots\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Step 2: Check for class imbalance (fraudulent vs non-fraudulent transactions)\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.countplot(x='Class', data=df_cleaned)\n",
        "plt.title('Class Distribution (0: Non-fraud, 1: Fraud)')\n",
        "plt.show()\n",
        "\n",
        "# Step 3: Correlation matrix\n",
        "plt.figure(figsize=(12,10))\n",
        "corr_matrix = df_cleaned.corr()\n",
        "sns.heatmap(corr_matrix, annot=False, cmap='coolwarm')\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Step 4: Distribution of the transaction amounts\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.histplot(df_cleaned['Amount'], bins=50, kde=True)\n",
        "plt.title('Distribution of Transaction Amounts')\n",
        "plt.show()\n",
        "\n",
        "# Step 5: Time feature analysis (if 'Time' is a feature in your dataset)\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.histplot(df_cleaned['Time'], bins=50, kde=True)\n",
        "plt.title('Distribution of Transaction Times')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "DYNreyAnL-Su"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Engineering"
      ],
      "metadata": {
        "id": "ygzoqb-cM4XI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Engineering: Transaction Velocity\n",
        "df_cleaned['Transaction_Velocity'] = df_cleaned.groupby('Time')['Amount'].transform(lambda x: x.rolling(window=10, min_periods=1).sum())\n",
        "\n",
        "# Inspect new feature\n",
        "print(df_cleaned[['Time', 'Amount', 'Transaction_Velocity']].head())\n"
      ],
      "metadata": {
        "id": "OgK5ysn1NFdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "b. Handling Imbalance: Fraudulent transactions are usually much less frequent, so you'll need to handle this class imbalance. You can use techniques like SMOTE (Synthetic Minority Over-sampling Technique) or undersampling. Here's an example using SMOTE:"
      ],
      "metadata": {
        "id": "npNehbz0Naox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install imbalanced-learn library (if needed)\n",
        "!pip install -U imbalanced-learn\n",
        "\n",
        "# Step 2: Import SMOTE and apply to balance the classes\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Separate features and target\n",
        "X = df_cleaned.drop('Class', axis=1)  # Features\n",
        "y = df_cleaned['Class']  # Target (fraud or non-fraud)\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Apply SMOTE to balance the training set\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Check the class distribution after SMOTE\n",
        "print(\"Class distribution before SMOTE:\", y_train.value_counts())\n",
        "print(\"Class distribution after SMOTE:\", y_train_sm.value_counts())\n"
      ],
      "metadata": {
        "id": "ET62BvYRNdDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Model Training\n",
        "Now that your data is clean, features are engineered, and the class imbalance is addressed, you're ready for model training. Hereâ€™s a quick code to train a Random Forest classifier as an example:"
      ],
      "metadata": {
        "id": "j458LRyhN0mB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Step 1: Train a Random Forest classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train_sm, y_train_sm)\n",
        "\n",
        "# Step 2: Make predictions\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Step 3: Evaluate the model\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "qrn0y3CON9qK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "LcrBe7JmP066"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [10, 20, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n",
        "grid_search.fit(X_train_sm, y_train_sm)\n",
        "\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n"
      ],
      "metadata": {
        "id": "hNtyByPBQHl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HhxxrMFHY_2h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensemble Models"
      ],
      "metadata": {
        "id": "YXD51EhhSsvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "estimators = [\n",
        "    ('rf', RandomForestClassifier(n_estimators=100)),\n",
        "    ('svc', SVC(probability=True))\n",
        "]\n",
        "\n",
        "stack_model = StackingClassifier(estimators=estimators, final_estimator=RandomForestClassifier())\n",
        "stack_model.fit(X_train_sm, y_train_sm)\n",
        "\n",
        "y_pred_stack = stack_model.predict(X_test)\n",
        "print(confusion_matrix(y_test, y_pred_stack))\n",
        "print(classification_report(y_test, y_pred_stack))\n"
      ],
      "metadata": {
        "id": "uDtEfx_pS2XW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Explainability: To make the model explainable, especially in a financial fraud context, you can use SHAP or LIME to interpret the feature importance for each prediction"
      ],
      "metadata": {
        "id": "PKxUEaMMS74i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "\n",
        "explainer = shap.TreeExplainer(rf)\n",
        "shap_values = explainer.shap_values(X_test)\n",
        "\n",
        "shap.summary_plot(shap_values[1], X_test)  # For class 1 (fraud)\n"
      ],
      "metadata": {
        "id": "O0H_5J2-S9Jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross-Validation: Use cross-validation to evaluate the modelâ€™s performance across multiple folds of the data, ensuring that the model generalizes well and isn't overfitting."
      ],
      "metadata": {
        "id": "E4HNjwx4TNtP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "scores = cross_val_score(rf, X_train_sm, y_train_sm, cv=5, scoring='accuracy')\n",
        "print(f\"Cross-Validation Accuracy: {scores.mean()}\")\n"
      ],
      "metadata": {
        "id": "DqHPMIzaTQPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Ethical Considerations\n",
        "Ethical issues are key when implementing AI models, particularly in sensitive areas like fraud detection. The major concerns include bias and fairness, transparency, and customer privacy.\n",
        "\n",
        "a. Bias and Fairness:\n",
        "Problem: AI models can unintentionally exhibit bias, especially when the training data contains imbalances. For example, if certain demographics are underrepresented in the data, the model may perform poorly on those groups.\n",
        "Solution: Regularly check for bias in your modelâ€™s predictions and use fairness metrics such as Demographic Parity, Equalized Odds, or Disparate Impact.\n",
        "Example to detect bias:"
      ],
      "metadata": {
        "id": "-ip94NcdZBkL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for bias in fraud detection predictions\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Let's assume you have demographic data available (like gender, age groups, etc.)\n",
        "# For example, you could check performance across gender:\n",
        "# df_cleaned['Gender'] is a column in your dataset representing demographic group\n",
        "for group in df_cleaned['Gender'].unique():\n",
        "    group_indices = df_cleaned[df_cleaned['Gender'] == group].index\n",
        "    y_true_group = y_test.loc[group_indices]\n",
        "    y_pred_group = y_pred.loc[group_indices]\n",
        "\n",
        "    print(f\"Confusion Matrix for Gender Group {group}:\")\n",
        "    print(confusion_matrix(y_true_group, y_pred_group))\n",
        "\n",
        "# Further analysis could calculate fairness metrics for each demographic group\n"
      ],
      "metadata": {
        "id": "cP_M6_2DZK0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "b. Transparency:\n",
        "Problem: Stakeholders (banks, regulators, and customers) need to understand how decisions are made by the AI model, especially in cases where a transaction is flagged as fraudulent.\n",
        "Solution: Use explainability tools like SHAP or LIME to provide instance-level explanations of why a transaction was flagged as fraud."
      ],
      "metadata": {
        "id": "CkFLqkLSZEiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "\n",
        "explainer = shap.TreeExplainer(rf)\n",
        "shap_values = explainer.shap_values(X_test)\n",
        "shap.summary_plot(shap_values[1], X_test)  # For class 1 (fraud)\n",
        "\n",
        "# You can also explain individual predictions:\n",
        "sample = X_test.iloc[0]\n",
        "shap.force_plot(explainer.expected_value[1], shap_values[1][0], sample)\n"
      ],
      "metadata": {
        "id": "dpID33yRZiwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "c. Customer Privacy:\n",
        "Problem: Your fraud detection model may require sensitive personal data (e.g., transaction history, location). This raises privacy concerns, especially with regulations like GDPR (in Europe) and CCPA (in California).\n",
        "Solution: Ensure compliance with these regulations by anonymizing personal data, using encryption, and allowing customers to opt-out of data collection."
      ],
      "metadata": {
        "id": "0ZpiBosOZZuT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Anonymizing sensitive data by hashing customer IDs\n",
        "import hashlib\n",
        "\n",
        "df_cleaned['Customer_ID_Hashed'] = df_cleaned['Customer_ID'].apply(lambda x: hashlib.sha256(str(x).encode()).hexdigest())\n",
        "\n",
        "# Drop or mask any personal identifiers before saving or transmitting the data\n",
        "df_cleaned.drop(columns=['Customer_ID'], inplace=True)\n"
      ],
      "metadata": {
        "id": "ChhQ6EacZwAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. IT Security\n",
        "Your AI model must be secure against external threats such as hacking, data breaches, and adversarial attacks.\n",
        "\n",
        "a. Data Encryption:\n",
        "Problem: Sensitive transaction and customer data need to be protected, especially during transmission between systems or storage.\n",
        "Solution: Use encryption protocols (e.g., AES, TLS) to protect data at rest and in transit.ction"
      ],
      "metadata": {
        "id": "Z-zClGznZyk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from cryptography.fernet import Fernet\n",
        "\n",
        "# Generate key for encryption\n",
        "key = Fernet.generate_key()\n",
        "cipher_suite = Fernet(key)\n",
        "\n",
        "# Encrypt sensitive columns (e.g., 'Transaction_Amount')\n",
        "df_cleaned['Transaction_Amount_Encrypted'] = df_cleaned['Amount'].apply(lambda x: cipher_suite.encrypt(str(x).encode()))\n",
        "\n",
        "# To decrypt the data (in deployment environment):\n",
        "# df_cleaned['Transaction_Amount_Decrypted'] = df_cleaned['Transaction_Amount_Encrypted'].apply(lambda x: cipher_suite.decrypt(x).decode())\n"
      ],
      "metadata": {
        "id": "JBhYhsXDZ6BP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "b. Adversarial Attacks:\n",
        "Problem: Hackers could use adversarial attacks to manipulate your model's predictions, leading to incorrect fraud detection.\n",
        "Solution: Implement adversarial training or defenses like defensive distillation to make the model more robust."
      ],
      "metadata": {
        "id": "ES8m0cd8aCB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Example of generating adversarial examples using the Fast Gradient Sign Method (FGSM)\n",
        "def create_adversarial_pattern(input_data, input_label, model):\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(input_data)\n",
        "        prediction = model(input_data)\n",
        "        loss = tf.keras.losses.mean_squared_error(input_label, prediction)\n",
        "\n",
        "    # Get the gradients of the loss w.r.t the input image.\n",
        "    gradient = tape.gradient(loss, input_data)\n",
        "    # Get the sign of the gradients to create the perturbation\n",
        "    signed_grad = tf.sign(gradient)\n",
        "    return signed_grad\n",
        "\n",
        "# Applying it to test set (assuming model is trained in TensorFlow/Keras)\n",
        "perturbations = create_adversarial_pattern(X_test, y_test, model)\n",
        "adversarial_test_data = X_test + perturbations\n"
      ],
      "metadata": {
        "id": "vzHo1fCdaDxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Governance\n",
        "Fraud detection models must comply with relevant legal and regulatory requirements, especially in the financial services sector. Key governance concerns include compliance with financial regulations like AML (Anti-Money Laundering) laws and ensuring auditability of AI decisions.\n",
        "a. Regulatory Compliance:\n",
        "Problem: The model needs to comply with regulatory frameworks such as AML, KYC (Know Your Customer), and international fraud detection guidelines.\n",
        "Solution: Ensure that the model adheres to industry regulations by keeping decision logs and offering interpretability.\n",
        "b. Auditability:\n",
        "Problem: Financial institutions need to audit the modelâ€™s predictions for potential legal issues. They need to understand how fraud was detected, and this process must be auditable.\n",
        "Solution: Keep a detailed log of model decisions, including inputs, outputs, and the reasons why a transaction was flagged."
      ],
      "metadata": {
        "id": "YdM0yBylaHZg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Log the model's predictions for auditing purposes\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(filename='fraud_detection_audit.log', level=logging.INFO)\n",
        "\n",
        "for i, (input_data, prediction) in enumerate(zip(X_test.values, y_pred)):\n",
        "    logging.info(f\"Transaction {i}: Input={input_data}, Prediction={prediction}\")\n"
      ],
      "metadata": {
        "id": "J9GGbZlxaMt6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}